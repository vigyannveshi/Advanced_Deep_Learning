{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# **Generative Adversarial Network for FMNIST**\n",
    "* **Basic concepts learnt from: A Deep understanding of Deep Learning (with Python intro) - Mark X Cohen (Udemy) - https://www.udemy.com/course/deeplearning_x**\n",
    "* **Extended learning and understanding by VigyannVeshi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# basic deep learning libraries\n",
    "import numpy as np\n",
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import trvision and transformations libraries\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# import dataset/loader libraries\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpcmh-V8hIlw"
   },
   "outputs": [],
   "source": [
    "device = tr.device('cuda' if tr.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfZKI3EXBHL5"
   },
   "outputs": [],
   "source": [
    "# import data and apply transformations\n",
    "transform = T.Compose([ T.ToTensor(),\n",
    "                        T.Normalize(.5,.5),\n",
    "                       ])\n",
    "\n",
    "# import the data and simultaneously apply the transform\n",
    "dataset = tv.datasets.FashionMNIST(root='./data', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLedkm063gpO"
   },
   "outputs": [],
   "source": [
    "# list the categories\n",
    "print(dataset.classes)\n",
    "\n",
    "# pick three categories (leave one line uncommented)\n",
    "# classes2keep = [ 'Trouser','Sneaker','Pullover' ]\n",
    "classes2keep = [ 'Trouser','Sneaker', 'Sandal'  ]\n",
    "\n",
    "\n",
    "\n",
    "# find the corresponding data indices\n",
    "images2use = tr.Tensor()\n",
    "for i in range(len(classes2keep)):\n",
    "  classidx = dataset.classes.index(classes2keep[i])\n",
    "  images2use = tr.cat( (images2use,tr.where(dataset.targets==classidx)[0]), 0).type(tr.long)\n",
    "  print(f'Added class {classes2keep[i]} (index {classidx})')\n",
    "\n",
    "# now select just those images\n",
    "\n",
    "# transform to dataloaders\n",
    "batchsize   = 100\n",
    "sampler     = tr.utils.data.sampler.SubsetRandomSampler(images2use)\n",
    "data_loader = DataLoader(dataset,sampler=sampler,batch_size=batchsize,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkHHEUXt6J57"
   },
   "outputs": [],
   "source": [
    "# view some images\n",
    "# inspect a few random images\n",
    "\n",
    "X,y = next(iter(data_loader))\n",
    "\n",
    "fig,axs = plt.subplots(3,6,figsize=(10,6))\n",
    "\n",
    "for (i,ax) in enumerate(axs.flatten()):\n",
    "\n",
    "  # extract that image\n",
    "  pic = tr.squeeze(X.data[i])\n",
    "  pic = pic/2 + .5 # undo normalization\n",
    "  \n",
    "  # and its label\n",
    "  label = dataset.classes[y[i]]\n",
    "\n",
    "  # and show!\n",
    "  ax.imshow(pic,cmap='gray')\n",
    "  ax.text(14,0,label,ha='center',fontweight='bold',color='k',backgroundcolor='y')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT-TyZZDK9-9"
   },
   "outputs": [],
   "source": [
    "class discriminatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.fc1 = nn.Linear(28*28,256)\n",
    "    self.fc2 = nn.Linear(256,256)\n",
    "    self.out = nn.Linear(256,1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.leaky_relu( self.fc1(x) )\n",
    "    x = F.leaky_relu( self.fc2(x) )\n",
    "    x = self.out(x)\n",
    "    return tr.sigmoid( x )\n",
    "\n",
    "dnet = discriminatorNet()\n",
    "y = dnet(tr.randn(10,784))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alVVPOJiLTHB"
   },
   "outputs": [],
   "source": [
    "class generatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.fc1 = nn.Linear(64,256)\n",
    "    self.fc2 = nn.Linear(256,256)\n",
    "    self.out = nn.Linear(256,784)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.leaky_relu( self.fc1(x) )\n",
    "    x = F.leaky_relu( self.fc2(x) )\n",
    "    x = self.out(x)\n",
    "    return tr.tanh( x )\n",
    "\n",
    "\n",
    "gnet = generatorNet()\n",
    "y = gnet(tr.randn(10,64))\n",
    "plt.imshow(y[0,:].detach().squeeze().view(28,28));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFDbnRqeCPqy"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "lossfun = nn.BCELoss()\n",
    "\n",
    "# create instances of the networks and push to the GPU\n",
    "dnet = discriminatorNet().to(device)\n",
    "gnet = generatorNet().to(device)\n",
    "\n",
    "# same algo but different instances for the G and D networks\n",
    "d_optimizer = tr.optim.Adam(dnet.parameters(), lr=.0003)\n",
    "g_optimizer = tr.optim.Adam(gnet.parameters(), lr=.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83Ju8fDuUTBg"
   },
   "outputs": [],
   "source": [
    "# this cell takes ~13 mins with 50k batches\n",
    "num_epochs = int(50000/len(data_loader))\n",
    "\n",
    "losses = np.zeros((num_epochs*len(data_loader),2))\n",
    "lossi  = 0\n",
    "\n",
    "for epochi in range(num_epochs):\n",
    "\n",
    "  for data,_ in data_loader:\n",
    "    \n",
    "    # send data to GPU\n",
    "    data = data.view(batchsize,-1).to(device)\n",
    "\n",
    "\n",
    "    # labels used for real and fake images\n",
    "    real_labels = tr.ones(batchsize,1).to(device)\n",
    "    fake_labels = tr.zeros(batchsize,1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    ### ---------------- Train the discriminator ---------------- ###\n",
    "\n",
    "    # forward pass and loss for REAL pictures\n",
    "    pred_real   = dnet(data)                     # output of discriminator\n",
    "    d_loss_real = lossfun(pred_real,real_labels) # all labels are 1\n",
    "\n",
    "    # forward pass and loss for FAKE pictures\n",
    "    fake_images = gnet( tr.randn(batchsize,64).to(device) ) # output of generator\n",
    "    pred_fake   = dnet(fake_images)                            # pass through discriminator\n",
    "    d_loss_fake = lossfun(pred_fake,fake_labels)               # all labels are 0\n",
    "\n",
    "    # collect loss (using combined losses)\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    losses[lossi,0] = d_loss.item()\n",
    "\n",
    "    # backprop\n",
    "    d_optimizer.zero_grad()\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    ### ---------------- Train the generator ---------------- ###\n",
    "\n",
    "    # create fake images and compute loss\n",
    "    fake_images = gnet( tr.randn(batchsize,64).to(device) )\n",
    "    pred_fake   = dnet(fake_images)\n",
    "\n",
    "    # compute and collect loss\n",
    "    g_loss = lossfun(pred_fake,real_labels)\n",
    "    losses[lossi,1] = g_loss.item()\n",
    "\n",
    "    # backprop\n",
    "    g_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    # increment loss counter\n",
    "    lossi += 1\n",
    "\n",
    "\n",
    "  # print out a status message\n",
    "  msg = f'Finished epoch {epochi+1}/{num_epochs}'\n",
    "  sys.stdout.write('\\r' + msg)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDZ3fTP3BUPq"
   },
   "outputs": [],
   "source": [
    "# create a 1D smoothing filter\n",
    "def smooth(x,k=15):\n",
    "  return np.convolve(x,np.ones(k)/k,mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1C0qAf9kN7mi"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
    "\n",
    "ax[0].plot(smooth(losses[:,0]))\n",
    "ax[0].plot(smooth(losses[:,1]))\n",
    "ax[0].set_xlabel('Batches')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].legend(['Discrimator','Generator'])\n",
    "# ax[0].set_xlim([4000,6000])\n",
    "\n",
    "ax[1].plot(losses[::5,0],losses[::5,1],'k.',alpha=.1)\n",
    "ax[1].set_xlabel('Discriminator loss')\n",
    "ax[1].set_ylabel('Generator loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzCz1UqGCP8T"
   },
   "outputs": [],
   "source": [
    "# generate the images from the generator network\n",
    "gnet.eval()\n",
    "fake_data = gnet(tr.randn(18,64).to(device)).cpu()\n",
    "\n",
    "# and visualize...\n",
    "fig,axs = plt.subplots(3,6,figsize=(12,6))\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(fake_data[i,:,].detach().view(28,28),cmap='gray')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.suptitle(classes2keep,y=.95,fontweight='bold')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNU2Lij0HspBG3wojeWpZtC",
   "collapsed_sections": [],
   "name": "DUDL_GAN_codeChallengeFMNIST.ipynb",
   "provenance": [
    {
     "file_id": "1t-RwrGO8-BWLf66uZZvWyFwAXjslhFMe",
     "timestamp": 1620794140775
    },
    {
     "file_id": "1W9fGz1EYzDhtHHpBYU6M2fEpi9Q1uXez",
     "timestamp": 1620754493662
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
