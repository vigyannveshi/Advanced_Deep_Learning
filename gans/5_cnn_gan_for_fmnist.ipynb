{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN - Generative Adversarial Network for FMNIST**\n",
    "* **Basic concepts learnt from: A Deep understanding of Deep Learning (with Python intro) - Mark X Cohen (Udemy) - https://www.udemy.com/course/deeplearning_x**\n",
    "* **Extended learning and understanding by VigyannVeshi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important libraries\n",
    "\n",
    "# basic deep learning libraries\n",
    "import numpy as np\n",
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import torchvision and transformations libraries\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# import dataset/loader libraries\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=tr.device('cuda' if tr.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transform = T.Compose([ T.ToTensor(),\n",
    "                        T.Resize(64),\n",
    "                        T.Normalize(.5,.5),\n",
    "                       ])\n",
    "\n",
    "# import the data and simultaneously apply the transform\n",
    "dataset = tv.datasets.FashionMNIST(root='./data', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the categories\n",
    "print(dataset.classes)\n",
    "\n",
    "# pick three categories (leave one line uncommented)\n",
    "classes2keep = [ 'Trouser','Sneaker','Pullover' ]\n",
    "# classes2keep = [ 'Trouser','Sneaker', 'Sandal'  ]\n",
    "\n",
    "\n",
    "\n",
    "# find the corresponding data indices\n",
    "images2use = tr.Tensor()\n",
    "for i in range(len(classes2keep)):\n",
    "  classidx = dataset.classes.index(classes2keep[i])\n",
    "  images2use = tr.cat( (images2use,tr.where(dataset.targets==classidx)[0]), 0).type(tr.long)\n",
    "  print(f'Added class {classes2keep[i]} (index {classidx})')\n",
    "\n",
    "# now select just those images\n",
    "\n",
    "# transform to dataloaders\n",
    "batchsize   = 100\n",
    "sampler     = tr.utils.data.sampler.SubsetRandomSampler(images2use)\n",
    "data_loader = DataLoader(dataset,sampler=sampler,batch_size=batchsize,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some images\n",
    "# inspect a few random images\n",
    "\n",
    "X,y = next(iter(data_loader))\n",
    "\n",
    "fig,axs = plt.subplots(3,6,figsize=(10,6))\n",
    "\n",
    "for (i,ax) in enumerate(axs.flatten()):\n",
    "\n",
    "  # extract that image\n",
    "  pic = tr.squeeze(X.data[i])\n",
    "  pic = pic/2 + .5 # undo normalization\n",
    "  \n",
    "  # and its label\n",
    "  label = dataset.classes[y[i]]\n",
    "\n",
    "  # and show!\n",
    "  ax.imshow(pic,cmap='gray')\n",
    "  ax.text(14,0,label,ha='center',fontweight='bold',color='k',backgroundcolor='y')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture and meta-parameter choices were inspired by https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator and generator\n",
    "\n",
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1,64,4,2,1,bias=False)\n",
    "        self.conv2 = nn.Conv2d(64,128,4,2,1,bias=False)\n",
    "        self.conv3 = nn.Conv2d(128,256,4,2,1,bias=False)\n",
    "        self.conv4 = nn.Conv2d(256,512,4,2,1,bias=False)\n",
    "        self.conv5 = nn.Conv2d(512,1,4,1,0,bias=False)\n",
    "\n",
    "        # batchnormalization\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4= nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.leaky_relu(self.conv1(x),0.2)\n",
    "        x=F.leaky_relu(self.conv2(x),0.2)\n",
    "        x=self.bn2(x)\n",
    "        x=F.leaky_relu(self.conv3(x),0.2)\n",
    "        x=self.bn3(x)\n",
    "        x=F.leaky_relu(self.conv4(x),0.2)\n",
    "        x=self.bn4(x)\n",
    "        return tr.sigmoid(self.conv5(x)).view(-1,1)\n",
    "\n",
    "dnet=DiscriminatorNet()\n",
    "y=dnet(tr.randn(10,1,64,64))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolutional layers\n",
    "        self.conv1=nn.ConvTranspose2d(100,512,4,1,0,bias=False)\n",
    "        self.conv2=nn.ConvTranspose2d(512,256,4,2,1,bias=False)\n",
    "        self.conv3=nn.ConvTranspose2d(256,128,4,2,1,bias=False)\n",
    "        self.conv4=nn.ConvTranspose2d(128,64,4,2,1,bias=False)\n",
    "        self.conv5=nn.ConvTranspose2d(64,1,4,2,1,bias=False)\n",
    "\n",
    "        # batchnorm\n",
    "        self.bn1=nn.BatchNorm2d(512)\n",
    "        self.bn2=nn.BatchNorm2d(256)\n",
    "        self.bn3=nn.BatchNorm2d(128)\n",
    "        self.bn4=nn.BatchNorm2d(64)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.bn1(self.conv1(x)))\n",
    "        x=F.relu(self.bn2(self.conv2(x)))\n",
    "        x=F.relu(self.bn3(self.conv3(x)))\n",
    "        x=F.relu(self.bn4(self.conv4(x)))\n",
    "        return tr.tanh(self.conv5(x))\n",
    "    \n",
    "gnet=GeneratorNet()\n",
    "y=gnet(tr.randn(10,100,1,1))\n",
    "plt.imshow(y[0,:,:,:].squeeze().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "lossfun=nn.BCELoss()\n",
    "\n",
    "dnet=DiscriminatorNet().to(device)\n",
    "gnet=GeneratorNet().to(device)\n",
    "\n",
    "d_optimizer=tr.optim.Adam(dnet.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "g_optimizer=tr.optim.Adam(gnet.parameters(),lr=0.0002,betas=(0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs (expressed in number of batches)\n",
    "num_epochs = int(2500/len(data_loader))\n",
    "\n",
    "losses  = []\n",
    "disDecs = []\n",
    "\n",
    "for epochi in range(num_epochs):\n",
    "\n",
    "  for data,_ in data_loader:\n",
    "    \n",
    "    # send data to GPU\n",
    "    data = data.to(device)\n",
    "\n",
    "    # create labels for real and fake images\n",
    "    real_labels = tr.ones(batchsize,1).to(device)\n",
    "    fake_labels = tr.zeros(batchsize,1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    ### ---------------- Train the discriminator ---------------- ###\n",
    "\n",
    "    # forward pass and loss for REAL pictures\n",
    "    pred_real   = dnet(data)                     # output of discriminator\n",
    "    d_loss_real = lossfun(pred_real,real_labels) # all labels are 1\n",
    "\n",
    "    # forward pass and loss for FAKE pictures\n",
    "    fake_data   = tr.randn(batchsize,100,1,1).to(device) # random numbers to seed the generator\n",
    "    fake_images = gnet(fake_data)                           # output of generator\n",
    "    pred_fake   = dnet(fake_images)                         # pass through discriminator\n",
    "    d_loss_fake = lossfun(pred_fake,fake_labels)            # all labels are 0\n",
    "\n",
    "    # collect loss (using combined losses)\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    # backprop\n",
    "    d_optimizer.zero_grad()\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    ### ---------------- Train the generator ---------------- ###\n",
    "\n",
    "    # create fake images and compute loss\n",
    "    fake_images = gnet( tr.randn(batchsize,100,1,1).to(device) )\n",
    "    pred_fake   = dnet(fake_images)\n",
    "\n",
    "    # compute loss\n",
    "    g_loss = lossfun(pred_fake,real_labels)\n",
    "\n",
    "    # backprop\n",
    "    g_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "\n",
    "    # collect losses and discriminator decisions\n",
    "    losses.append([d_loss.item(),g_loss.item()])\n",
    "    \n",
    "    d1 = tr.mean((pred_real>.5).float()).detach().cpu()\n",
    "    d2 = tr.mean((pred_fake>.5).float()).detach().cpu()\n",
    "    disDecs.append([d1,d2])\n",
    "\n",
    "\n",
    "  # print out a status message\n",
    "  msg = f'Finished epoch {epochi+1}/{num_epochs}'\n",
    "  sys.stdout.write('\\r' + msg)\n",
    "\n",
    "\n",
    "# convert performance from list to numpy array\n",
    "losses  = np.array(losses)\n",
    "disDecs = np.array(disDecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 1D smoothing filter  \n",
    "def smooth(x,k=15):\n",
    "    return np.convolve(x,np.ones(k)/k,mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(smooth(losses[:,0]))\n",
    "ax[0].plot(smooth(losses[:,1]))\n",
    "ax[0].set_xlabel('Batches')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title(\"Model Loss\")\n",
    "ax[0].legend(['Discriminator','Generator'])\n",
    "\n",
    "ax[1].plot(losses[:,0],losses[:,1],'k.',alpha=0.1)\n",
    "ax[1].set_xlabel('Discriminator Loss')\n",
    "ax[1].set_ylabel(\"Generator loss\")\n",
    "\n",
    "ax[2].plot(smooth(disDecs[:,0]))\n",
    "ax[2].plot(smooth(disDecs[:,1]))\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('Probability (\"real\")')\n",
    "ax[2].set_title('Discriminator output')\n",
    "ax[2].legend(['Real','Fake'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the images from the generator network\n",
    "gnet.eval()\n",
    "fake_data = gnet( tr.randn(batchsize,100,1,1).to(device) ).cpu()\n",
    "\n",
    "# and visualize...\n",
    "fig,axs = plt.subplots(3,6,figsize=(12,6))\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(fake_data[i,:,].detach().squeeze(),cmap='gray')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.suptitle(classes2keep,y=.95,fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Explorations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) I've mentioned before that GANs can be quite sensitive to subtle changes in model architecture. Try running the code again with exactly one change: Set the Adam 'betas' parameters to their default values (simply remove that argument from the code. How much of an impact does this have on the results? More generally, these sensitivities can be frustrating when trying to build new models; the best thing to do is search the web for similar kinds of models and be inspired by their decisision (but don't assume that a model is good just because it's on the web!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
