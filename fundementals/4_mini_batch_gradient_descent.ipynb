{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mini-Batch Gradient Descent**\n",
    "\n",
    "Learnt it using:<br>\n",
    "**Reference: https://www.youtube.com/watch?v=_scscQ4HVTY**<br><br>\n",
    "**Note:** *Scroll down for the code part, previous codes are added for comparison*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "X,y=load_diabetes(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
      "151.88331005254167\n"
     ]
    }
   ],
   "source": [
    "reg=LinearRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399338661568968"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating own class for multiple variable linear regression using gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "\n",
    "    def __init__(self,learning_rate=0.01, epochs=100):\n",
    "        self.coef_=None\n",
    "        self.intercept_=None\n",
    "        self.lr=learning_rate\n",
    "        self.epochs=epochs\n",
    "\n",
    "    def fit(self, X_train,y_train):\n",
    "        # initialize your coefs\n",
    "        self.intercept_=0\n",
    "        self.coef_=np.ones(shape=X_train.shape[1])\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            # update all the coefs and intercept\n",
    "            y_hat=self.intercept_+(X_train) @ (self.coef_)\n",
    "            intercept_der=-2*np.mean(y_train-y_hat)\n",
    "            # vectorization (we don't need to use a loop)\n",
    "            coef_der=(-2/X_train.shape[0])*((y_train-y_hat).T@(X_train))\n",
    "            # updating intercept\n",
    "            self.intercept_=self.intercept_-self.lr*intercept_der\n",
    "            # updating coefficients\n",
    "            self.coef_=self.coef_-self.lr*coef_der\n",
    "\n",
    "        print(self.intercept_,self.coef_)\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        return self.intercept_+(X_test) @ (self.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.01351687661833 [  14.38990585 -173.7235727   491.54898524  323.91524824  -39.32648042\n",
      " -116.01061213 -194.04077415  103.38135565  451.63448787   97.57218278]\n",
      "Time taken is 0.01743149757385254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4534503034722803"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdr=GDRegressor(epochs=1000,learning_rate=0.5)\n",
    "start=time.time()\n",
    "gdr.fit(X_train,y_train)\n",
    "print(f\"Time taken is {time.time()-start}\")\n",
    "y_pred=gdr.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating own class for multiple variable linear regression using stochastic gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Schedular\n",
    "t0,t1=5,50\n",
    "def learning_rate(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "\n",
    "class SGDRegressor:\n",
    "\n",
    "    def __init__(self,learning_rate=0.01, epochs=100):\n",
    "        self.coef_=None\n",
    "        self.intercept_=None\n",
    "        self.lr=learning_rate\n",
    "        self.epochs=epochs\n",
    "\n",
    "    def fit(self, X_train,y_train):\n",
    "        # initialize your coefs\n",
    "        self.intercept_=0\n",
    "        self.coef_=np.ones(shape=X_train.shape[1])\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(X_train.shape[0]):\n",
    "                # making learning rate as a function of epochs\n",
    "                # self.lr=learning_rate(i*X.shape[0]+j) \n",
    "\n",
    "                idx=np.random.randint(0,X_train.shape[0]) # high not included\n",
    "                y_hat=(X_train[idx]@self.coef_)+self.intercept_ # a scalar\n",
    "                intercept_der=-2*(y_train[idx]-y_hat)\n",
    "                coef_der=-2*(y_train[idx]-y_hat)*X_train[idx]\n",
    "                self.intercept_=self.intercept_-(self.lr*intercept_der)\n",
    "                self.coef_=self.coef_-(self.lr*coef_der)\n",
    "\n",
    "        print(self.intercept_,self.coef_)\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        return self.intercept_+(X_test) @ (self.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155.8458989041669 [   3.83888249 -199.3026714   531.96715424  304.32184666  -78.49962735\n",
      " -109.7301724  -205.33664563   83.2952533   518.55627547   49.37643967]\n",
      "Time taken is 0.10808205604553223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44999022702474756"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd=SGDRegressor(epochs=40,learning_rate=0.1)\n",
    "start=time.time()\n",
    "sgd.fit(X_train,y_train)\n",
    "print(f\"Time taken is {time.time()-start}\")\n",
    "y_pred=sgd.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is 0.002301454544067383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43035854274369656"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg=SGDRegressor(max_iter=100,learning_rate='constant',eta0=0.01)\n",
    "start=time.time()\n",
    "reg.fit(X_train,y_train)\n",
    "print(f\"Time taken is {time.time()-start}\")\n",
    "y_pred=reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating own class for multiple variable linear regression using Mini Batch gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class MBGDRegressor:\n",
    "\n",
    "    def __init__(self,batch_size,learning_rate=0.01, epochs=100):\n",
    "        self.coef_=None\n",
    "        self.intercept_=None\n",
    "        self.lr=learning_rate\n",
    "        self.epochs=epochs\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def fit(self, X_train,y_train):\n",
    "        # initialize your coefs\n",
    "        self.intercept_=0\n",
    "        self.coef_=np.ones(shape=X_train.shape[1])\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                idx=random.sample(range(X_train.shape[0]),self.batch_size)\n",
    "\n",
    "                # update all the coefs and intercept\n",
    "                y_hat=self.intercept_+(X_train[idx]) @ (self.coef_)\n",
    "                intercept_der=-2*np.mean(y_train[idx]-y_hat)\n",
    "                \n",
    "                # vectorization (we don't need to use a loop)\n",
    "                coef_der=(-2/X_train.shape[0])*((y_train[idx]-y_hat).T@(X_train[idx]))\n",
    "                # updating intercept\n",
    "                self.intercept_=self.intercept_-self.lr*intercept_der\n",
    "                # updating coefficients\n",
    "                self.coef_=self.coef_-self.lr*coef_der\n",
    "\n",
    "        print(self.intercept_,self.coef_)\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        return self.intercept_+(X_test) @ (self.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.2583828237332 [  -9.0406545  -196.53780954  532.00185153  337.67970223 -149.53959471\n",
      "  -44.56445989 -163.01405556   57.4291265   574.54702299   50.77920183]\n",
      "Time taken is 4.348752975463867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44268825143876134"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbgd=MBGDRegressor(batch_size=int(X_train.shape[0]/10),epochs=10000,learning_rate=0.4)\n",
    "start=time.time()\n",
    "mbgd.fit(X_train,y_train)\n",
    "print(f\"Time taken is {time.time()-start}\")\n",
    "y_pred=mbgd.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**implementating minibatch gradient descent using scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is 0.025076866149902344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45315605473942033"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd=SGDRegressor(learning_rate='constant',eta0=0.2)\n",
    "batch_size=35\n",
    "epochs=100\n",
    "start=time.time()\n",
    "for i in range(epochs):\n",
    "    idx=random.sample(range(X_train.shape[0]),batch_size)\n",
    "    sgd.partial_fit(X_train[idx],y_train[idx])\n",
    "print(f\"Time taken is {time.time()-start}\")\n",
    "y_pred=sgd.predict(X_test)\n",
    "r2_score(y_test,y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
