{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOax0KSYg9Ff"
      },
      "source": [
        "# **Train on EMNIST (for letters) and Transfer for MNIST**\n",
        "* **Basic concepts learnt from: A Deep understanding of Deep Learning (with Python intro) - Mark X Cohen (Udemy) - https://www.udemy.com/course/deeplearning_x**\n",
        "* **Extended learning and understanding by VigyannVeshi**\n",
        "* **EMINST Dataset**\n",
        "    * Dataset containing (A-Z, a-z, 0-9) handwritten character images of size (28,28) grayscale\n",
        "    * Total 814255 characters\n",
        "\n",
        "* **MNIST:**\n",
        "    * MNIST - Modified National Institute of Standards and Technology\n",
        "    * Image Size: 28x28 pixels\n",
        "    * Total Images: 70,000 (60,000 training, 10,000 testing)\n",
        "    * Channels: Grayscale, 1 channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkffNUg2hN74"
      },
      "outputs": [],
      "source": [
        "# basic deep learning libraries\n",
        "import numpy as np\n",
        "import torch as tr\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "\n",
        "# import dataset/loader libraries\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import summary libraries for model information\n",
        "from torchsummary import summary\n",
        "\n",
        "# import plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "# extra imports\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_________________________\n",
        "**Accessing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/rudraksha14/Desktop/RAY_RISE_ABOVE_YOURSELF/Programming/advanced_deep_learning\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/rudraksha14/Desktop/RAY_RISE_ABOVE_YOURSELF/Programming/advanced_deep_learning/datasets\n"
          ]
        }
      ],
      "source": [
        "cd datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "______________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdDuOp-JhnFR",
        "outputId": "a959a772-9c10-4c20-f6c1-7462f61226cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.1%"
          ]
        }
      ],
      "source": [
        "# Import and inspect the data\n",
        "# cdata=tv.datasets.EMNIST(root='emnist',split='letters')\n",
        "cdata=tv.datasets.EMNIST(root='emnist',split='letters',download=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2yVRm9Xh5MT",
        "outputId": "68829387-4cd8-4362-998c-b9a7d18c7105"
      },
      "outputs": [],
      "source": [
        "#inspect the data\n",
        "\n",
        "# the categories (but how many letters?)\n",
        "\n",
        "print(cdata.classes)\n",
        "print(str(len(cdata.classes))+'classes')\n",
        "\n",
        "print(\"\\nData Size:\")\n",
        "print(cdata.data.shape)\n",
        "\n",
        "# transform to 4D tensor for conv layers (and transform from int8 to float)\n",
        "images=cdata.data.view([124800,1,28,28]).float()\n",
        "print('\\nTensor Data: ')\n",
        "print(images.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSolx96k_Q5L",
        "outputId": "2f89a0b0-ac1c-4623-eca9-5e44acab3b5d"
      },
      "outputs": [],
      "source": [
        "# brief aside: class 'N/A' doesn't exist in the data\n",
        "print(tr.sum(cdata.targets==0))\n",
        "\n",
        "# however, it causes problems in one-hot encoding...\n",
        "tr.unique(cdata.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVcU9wVm_pET",
        "outputId": "01b9fd22-ef2d-4db7-9c45-c916cd3c01fa"
      },
      "outputs": [],
      "source": [
        "cdata.class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ce0m8Bl_tk0",
        "outputId": "cdc85a7f-4363-4f89-e9eb-a0f0b4fb003a"
      },
      "outputs": [],
      "source": [
        "# so therefore we'll eliminate 'N/A' and subtract 1 from the original labels\n",
        "\n",
        "# remove the first class category\n",
        "letterCategories = cdata.classes[1:]\n",
        "\n",
        "# relabel labels to start at 0\n",
        "labels=copy.deepcopy(cdata.targets)-1\n",
        "print(labels.shape)\n",
        "\n",
        "print(tr.sum(labels==0))\n",
        "print(tr.unique(labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "id": "gur6FvsYAp_H",
        "outputId": "f2b52850-0eb0-4862-c744-7f90dd9516ea"
      },
      "outputs": [],
      "source": [
        "# next issue: do we need to normalize the images?\n",
        "plt.hist(images[:10,:,:,:].view(1,-1).detach(),40)\n",
        "plt.title('Raw values')\n",
        "plt.show()\n",
        "\n",
        "# yarp.{Means YES!}\n",
        "images/=tr.max(images)\n",
        "plt.hist(images[:10,:,:,:].view(1,-1).detach(),40)\n",
        "plt.title('After Normalization!!')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "smhpcLZ0BQa4",
        "outputId": "7c70aa8f-996f-44c2-aa07-869c0d074ee7"
      },
      "outputs": [],
      "source": [
        "# visualization of some images\n",
        "fig,axs=plt.subplots(3,7,figsize=(13,6))\n",
        "\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "\n",
        "  # pick a random pic\n",
        "  whichpic=np.random.randint(images.shape[0])\n",
        "\n",
        "  # extract the image and its target letter\n",
        "  I=np.squeeze(images[whichpic,:,:])\n",
        "  letter=letterCategories[labels[whichpic]]\n",
        "\n",
        "  # visualize\n",
        "  ax.imshow(I.T,cmap='gray')\n",
        "  ax.set_title(f'The letter {letter}')\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p46bZ-LPChQX"
      },
      "outputs": [],
      "source": [
        "# Create train/test groups using Dataloader\n",
        "\n",
        "# step 2: use scikit-learn to split the data\n",
        "train_data,test_data,train_labels,test_labels=train_test_split(images,labels,test_size=0.1)\n",
        "\n",
        "# step 3: convert to Pytorch Datasets\n",
        "train_data=TensorDataset(train_data,train_labels)\n",
        "test_data= TensorDataset(test_data,test_labels)\n",
        "\n",
        "# step 4: translate into dataloader objects\n",
        "batchsize=32\n",
        "train_loader=DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "test_loader=DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDwTJMKTCrcI",
        "outputId": "6bf02c0f-3512-4321-b2b6-cd27eea499f9"
      },
      "outputs": [],
      "source": [
        "# check size (should be images x channels x width x height)\n",
        "print(train_loader.dataset.tensors[0].shape)\n",
        "print(train_loader.dataset.tensors[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxlU6iENC0el"
      },
      "outputs": [],
      "source": [
        "# creating a class for the model\n",
        "def createNet(printtoggle=False):\n",
        "    class mnistNet(nn.Module):\n",
        "        def __init__(self,printtoggle):\n",
        "            super().__init__()\n",
        "\n",
        "            ### convolution layers\n",
        "\n",
        "            # first convolution layer\n",
        "            self.conv1=nn.Conv2d(1,6,kernel_size=3,stride=1,padding=1)\n",
        "            self.bnorm1=nn.BatchNorm2d(6) # input the number of channels in this layers\n",
        "            ### output size: (28+2*1-3)/1 + 1 = 28 --> //2 = 14 (After maxpool (2x2))\n",
        "\n",
        "            # second convolutional layer\n",
        "            self.conv2=nn.Conv2d(6,6,kernel_size=3,stride=1,padding=1)\n",
        "            self.bnorm2=nn.BatchNorm2d(6) # input the number of channels in this layers\n",
        "            ### output size: (14+2*1-3)/1 + 1 = 14 --> //2 = 7 (After maxpool (2x2))\n",
        "\n",
        "\n",
        "            ### linear decision layer\n",
        "\n",
        "            # fully-connected layer\n",
        "            self.fc1=nn.Linear(7*7*6,50)\n",
        "\n",
        "            # output layer\n",
        "            self.out=nn.Linear(50,26)\n",
        "\n",
        "            # toggle for printing out tensor sizes during forward prop\n",
        "            self.print=printtoggle\n",
        "\n",
        "        # forward propogation\n",
        "        def forward(self,x):\n",
        "            print(f'Input: {x.shape}') if self.print else None\n",
        "\n",
        "            ### convolution -> maxpool -> relu (1)\n",
        "            x = F.leaky_relu(self.bnorm1(F.max_pool2d(self.conv1(x),2)))\n",
        "            print(f'Layer conv1/pool1: {x.shape}') if self.print else None\n",
        "\n",
        "            ### convolution -> maxpool -> relu (2)\n",
        "            x = F.leaky_relu(self.bnorm2(F.max_pool2d(self.conv2(x),2)))\n",
        "            print(f'Layer conv2/pool2: {x.shape}') if self.print else None\n",
        "\n",
        "            # reshape for linear layer\n",
        "            ### x.shape.numel() -->  gives total number of elements in the array/tuple\n",
        "            nUnits=x.shape.numel()/x.shape[0]\n",
        "            ### vectorizing\n",
        "            x=x.view(-1,int(nUnits))\n",
        "            print(f'Vectorize: {x.shape}') if self.print else None\n",
        "\n",
        "            # linear layers\n",
        "            x=F.leaky_relu(self.fc1(x))\n",
        "            print(f'Layer fc1: {x.shape}') if self.print else None\n",
        "            x=self.out(x)\n",
        "            print(f'Layer fc1: {x.shape}') if self.print else None\n",
        "\n",
        "            return x\n",
        "\n",
        "    # create the model instance\n",
        "    net=mnistNet(printtoggle)\n",
        "\n",
        "    # loss function\n",
        "    lossfun=nn.CrossEntropyLoss()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer=tr.optim.Adam(net.parameters(),lr=0.001)\n",
        "\n",
        "    return net,lossfun,optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQHxIKn1Fj5P",
        "outputId": "a74f0e51-107f-4b07-d52a-079a5eeffdfb"
      },
      "outputs": [],
      "source": [
        "# test the model with one batch\n",
        "net,lossfun,optimizer = createNet(True)\n",
        "\n",
        "X,y = next(iter(train_loader))\n",
        "yHat = net(X)\n",
        "\n",
        "print('\\nLabel size:')\n",
        "print(y.shape)\n",
        "\n",
        "# check size of output\n",
        "print('\\nOutput size:')\n",
        "print(yHat.shape)\n",
        "\n",
        "# # now let's compute the loss\n",
        "loss = lossfun(yHat,tr.squeeze(y))\n",
        "print(' ')\n",
        "print('Loss:')\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQPx4igiGu0V",
        "outputId": "7dfc7814-bf70-452c-8439-6c426280cbbe"
      },
      "outputs": [],
      "source": [
        "# setting up gpu\n",
        "# use GPU if available\n",
        "device = tr.device('cuda:0' if tr.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCk99N8HGYNX",
        "outputId": "8b760fb9-cc96-4cc2-e8e4-b7f2e978c9c8"
      },
      "outputs": [],
      "source": [
        "# count the total number of parameters in the model\n",
        "net.to(device)\n",
        "summary(net,(1,28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywP7SaMfG1bf"
      },
      "outputs": [],
      "source": [
        "# Create a function that trains the model\n",
        "\n",
        "def trainModel(n_epochs=10):\n",
        "    # create a new model\n",
        "    net,lossfun,optimizer=createNet()\n",
        "\n",
        "    # send the model to the GPU\n",
        "    net.to(device)\n",
        "\n",
        "    # initialize losses\n",
        "    trainLoss = tr.zeros(n_epochs)\n",
        "    testLoss = tr.zeros(n_epochs)\n",
        "    trainErr = tr.zeros(n_epochs)\n",
        "    testErr = tr.zeros(n_epochs)\n",
        "\n",
        "    # loop over epochs\n",
        "    for epochi in range(n_epochs):\n",
        "\n",
        "        # loop over training data batches\n",
        "        batchLoss=[]\n",
        "        batchErr=[]\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # push data to GPU\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # forward pass and loss\n",
        "            yHat=net(X)\n",
        "            loss=lossfun(yHat,y)\n",
        "\n",
        "            # backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # loss from this batch\n",
        "            batchLoss.append(loss.item())\n",
        "            batchErr.append(tr.mean((tr.argmax(yHat,axis=1)!=y).float()).item())\n",
        "\n",
        "            # end of batch loop ...\n",
        "\n",
        "        # and get the average losses across the batches\n",
        "        trainLoss[epochi]=np.mean(batchLoss)\n",
        "        trainErr[epochi]=100*np.mean(batchErr)\n",
        "\n",
        "        # test accuracy\n",
        "        X,y=next(iter(test_loader))\n",
        "        # push data to GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        with tr.no_grad(): # deactivates autograd\n",
        "            yHat=net(X)\n",
        "            loss=lossfun(yHat,y)\n",
        "\n",
        "        # compare the following really long lines of code to the training accuracy lines\n",
        "        testLoss[epochi]=loss.item()\n",
        "        testErr[epochi]=100*tr.mean((tr.argmax(yHat,axis=1)!=y).float()).item()\n",
        "\n",
        "        print(f\"{epochi+1}/{n_epochs} complete!\")\n",
        "    # end of epochs\n",
        "\n",
        "    # function output\n",
        "    return trainLoss,testLoss,trainErr,testErr,net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aGwS93tINgq",
        "outputId": "1f438454-78a5-425e-cf22-bace2ce45305"
      },
      "outputs": [],
      "source": [
        "# Run the model and show the results!\n",
        "# GPU ~ 1min; CPU~2.47 mins (Ryzen 7 5700u)\n",
        "trainLoss,testLoss,trainErr,testErr,net=trainModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "FFI0UD_nP8Fi",
        "outputId": "8f5b0577-f7d4-489e-b2ff-b5e2933d959d"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
        "\n",
        "ax[0].plot(trainLoss,'s-',label='Train')\n",
        "ax[0].plot(testLoss,'o-',label='Test')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss (Binary Cross Entropy)')\n",
        "ax[0].set_title('Model loss')\n",
        "\n",
        "ax[1].plot(trainErr,'s-',label='Train')\n",
        "ax[1].plot(testErr,'o-',label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Error rates (%)')\n",
        "ax[1].set_title(f'Final model test error rate: {testErr[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "3KapaodRQOkB",
        "outputId": "388842b7-ea81-4c76-af67-868c5262746f"
      },
      "outputs": [],
      "source": [
        "### visualize some images\n",
        "\n",
        "# extract X,y from test dataloader\n",
        "X,y = next(iter(test_loader))\n",
        "X = X.to(device) # push data to GPU\n",
        "y = y.to(device) # push data to GPU\n",
        "yHat = net(X)\n",
        "\n",
        "# pick some examples at random to show\n",
        "randex = np.random.choice(len(y),size=21,replace=False)\n",
        "\n",
        "# visualize some images\n",
        "fig,axs = plt.subplots(3,7,figsize=(15,6))\n",
        "\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "\n",
        "  # extract the image and its target letter\n",
        "  I = np.squeeze( X[randex[i],0,:,:] ).cpu() # .cpu() to transfer back from GPU!\n",
        "  trueLetter = letterCategories[ y[randex[i]] ]\n",
        "  predLetter = letterCategories[ tr.argmax(yHat[randex[i],:]) ]\n",
        "\n",
        "  # color-code the accuracy (using ternary operator)\n",
        "  col = 'gray' if trueLetter==predLetter else 'hot'\n",
        "\n",
        "  # visualize\n",
        "  ax.imshow(I.T,cmap=col)\n",
        "  ax.set_title('True %s, predicted %s' %(trueLetter,predLetter),fontsize=10)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "id": "2ccSwXdiQuUR",
        "outputId": "fed9823d-4b09-428e-bcdb-3353cce446d7"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as skm\n",
        "\n",
        "# compute the confusion matrix\n",
        "C = skm.confusion_matrix(y.cpu(),tr.argmax(yHat.cpu(),axis=1),normalize='true')\n",
        "\n",
        "# visualize it\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "plt.imshow(C,'Blues',vmax=.05)\n",
        "\n",
        "# make the plot look nicer\n",
        "plt.xticks(range(26),labels=letterCategories)\n",
        "plt.yticks(range(26),labels=letterCategories)\n",
        "plt.title('TEST confusion matrix')\n",
        "plt.xlabel('Predicted letter')\n",
        "plt.ylabel('True letter')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explorations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) I added batch normalization to the convolution layers, but not to the linear (fc*) layers. But linear layers also  benefit from batchnorm just like convolution layers do. Add it!\n",
        "\n",
        "# 2) In the next few videos, we will see whether we can improve the model's performance by experimenting with the number of layers, kernel size, and linear-layer units. Is there anything you could think of, other than these three features, that might help boost model performance?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
